{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 自己实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO sentecne length is:42068\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('../d2l-zh/data/ptb.zip', 'r') as zin:\n",
    "    zin.extractall('../d2l-zh/data')\n",
    "    \n",
    "with open('../d2l-zh/data/ptb/ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "\n",
    "print(\"INFO sentecne length is:{}\".format(len(raw_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO the whole token numbers is:885720\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter((lambda x: x[1]>5), counter.items()))\n",
    "\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] \n",
    "           for st in raw_dataset]\n",
    "num_tokens = sum(len(st) for st in dataset)\n",
    "print(\"INFO the whole token numbers is:{}\".format(num_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsampling to remove some redundent words\n",
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1- math.sqrt(1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @fucntion: get_centers_and_contexts. 从数据集中建立中心词与背景词\n",
    "# @params: dataset(整个词典）, max_window_size（每个中心词的window最大尺寸）\n",
    "# @return: centers, contexts 中心词list及其对以的背景词contexts list\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size), \n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "                           \n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n",
    "\n",
    "# @function: get_negatives. 实现负采样，重点获取一口contex window中K 个噪声词\n",
    "# @params: all_contexts（所有的contexts词语为idx二维list）, \n",
    "#          sampling_weights（噪声词采样词典中各个词的权重）, \n",
    "#          K（一个window窗口中噪声词的数量）\n",
    "# @return: all_negatives. 返回负采样后的数据，整个是二维的的数据\n",
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negtives, neg_condidates, i= [], [], 0\n",
    "    polulation = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negtives = []\n",
    "        while len(negtives) < len(contexts) * K:\n",
    "            if i == len(neg_condidates):\n",
    "                i, neg_condidates = 0, random.choices(\n",
    "                    polulation, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_condidates[i], i+1\n",
    "            if neg not in set(contexts):\n",
    "                negtives.append(neg)\n",
    "        all_negtives.append(negtives)\n",
    "    return all_negtives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token] # 论文中建议的噪声词的采样频率与总词频的0.75次方\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @function: batchfy. 对数据进行batch化处理,其中所有的context加negative会做定长度的处理，最大长度下做mask的处理来标示。\n",
    "# @para: data, 整个高训练的数据集包含：centers、contexts、negative\n",
    "# @return: centers（中心词的向量，二维）, context_negatives（补全到最大长度的context与negatives，二维）, \n",
    "#          mask（有效词掩码，二维）, labels（context词语掩码，二维—）\n",
    "def batchify(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, context_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        context_negatives += [context + negative + [0]*(max_len -cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (nd.array(centers).reshape((-1, 1)), nd.array(context_negatives),\n",
    "            nd.array(masks), nd.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: (512, 1)\n",
      "context_negatives shape: (512, 60)\n",
      "masks shape: (512, 60)\n",
      "labels shape: (512, 60)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "dataset = gdata.ArrayDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            batchify_fn=batchify, num_workers=num_workers)\n",
    "\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'context_negatives', 'masks', 'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP Gram\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = nd.batch_dot(v, u.swapaxes(1,2))\n",
    "    return pred\n",
    "\n",
    "embed_size = 100\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Embedding(input_dim=len(idx_to_token), output_dim=embed_size),\n",
    "        nn.Embedding(input_dim=len(idx_to_token), output_dim=embed_size))\n",
    "loss = gloss.SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "def train(net, lr, num_epochs):\n",
    "    ctx = d2l.try_gpu()\n",
    "    net.initialize(ctx=ctx, force_reinit=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate' : lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [data.as_in_context(ctx) for data in batch]\n",
    "            with autograd.record():\n",
    "                pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "                l = (loss(pred.reshape(label.shape), label, mask) * \n",
    "                  mask.shape[1] / mask.sum(axis=1))\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            l_sum += l.sum().asscalar()\n",
    "            n += l.size\n",
    "        print('epoch %d, loss %.2f, %.2fs' % (epoch + 1, l_sum / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.46, 81.64s\n",
      "epoch 2, loss 0.39, 79.47s\n",
      "epoch 3, loss 0.35, 83.27s\n",
      "epoch 4, loss 0.32, 93.20s\n",
      "epoch 5, loss 0.31, 91.95s\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.005, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data()\n",
    "    x = W(token_to_idx[query_token] \n",
    "    cos = nd.dot(W, x) / (nd.sum(W*W, axis=1)) * nd.sum(x *x) + 1e-9).sqrt()\n",
    "    topk = nd.topk(cos, k=k+1, ret_type='indices').asnumpy().astype('int32')\n",
    "    for i in topk[1:]:\n",
    "        print('cosine sim=%.3f: %s ' % (cos[i].asscalar(), (idx_to_token[i])))\n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
